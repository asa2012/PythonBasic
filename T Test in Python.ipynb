{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sample size \n",
    "N=10\n",
    "#Gaussian distributed data with mean=2 and var=1 \n",
    "a=np.random.randn(N)+2\n",
    "\n",
    "#Gaussian distributed data with mean=1 and var=1 \n",
    "b=np.random.randn(N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.50113651  0.10466505  1.51973519  4.12328775  1.52062184  1.24173954\n",
      "  2.76126578  1.3521573   1.71950547  2.10877214]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1775202403742222"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate standard deviation\n",
    "#calculate the variance to get the standard deviation\n",
    "print(a)\n",
    "import statistics\n",
    "statistics.stdev(a)#this is equivalent with excel function STDEV.S(A1:J1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for unbiased max likelihood estimate, we have to divide the var by N-1, and therefore the parameter\n",
    "var_a=a.var(ddof=1)\n",
    "var_b=b.var(ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0516276810507086"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#std deviation\n",
    "s=np.sqrt((var_a+var_b)/2)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate the t-statistics\n",
    "t=(a.mean()-b.mean())/(s*np.sqrt(2/N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compare wth critical t-value\n",
    "\n",
    "#degree of freedom \n",
    "df=2*N-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#p-value after comparison with the t\n",
    "p=1-stats.t.cdf(t,df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=4.73361989055\n",
      "p=0.000165785585608\n"
     ]
    }
   ],
   "source": [
    "print(\"t=\"+str(t))\n",
    "print(\"p=\"+str(2*p))\n",
    "\n",
    "#Note that we multiply the p value by 2 because its a twp tail t-test\n",
    "### You can see that after comparing the t statistic with the critical t value (computed internally) we get a good p value of 0.0005 and thus we reject the null hypothesis and thus it proves that the mean of the two distributions are different and statistically significant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 4.73361989055\n",
      "p = 0.000331571171215\n"
     ]
    }
   ],
   "source": [
    "## Cross Checking with the internal scipy function\n",
    "t2, p2 = stats.ttest_ind(a,b)\n",
    "print(\"t = \" + str(t2))\n",
    "print(\"p = \" + str(2*p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean value is: 0.0289497985251\n",
      "1.00533216026\n",
      "1.00633900294\n",
      "1.01069275246\n",
      "1.01271818884\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(12345678)\n",
    "rvs1=stats.norm.rvs(loc=6,scale=10,size=500)#stats.norm.rvs, loc means for mean vlaue, scale means for stdev, size means # of samples \n",
    "rvs1.mean()\n",
    "rvs2=stats.norm.rvs(loc=5, scale=10, size=500)\n",
    "\n",
    "norm_rvs=stats.norm.rvs(loc=0,scale=1,size=500)\n",
    "print('mean value is: '+str(norm_rvs.mean()))\n",
    " \n",
    "print(np.std(norm_rvs))#same as stdev.p\n",
    "print(np.std(norm_rvs,ddof=1))#same as stdev.s\n",
    "print(np.var(norm_rvs)) #this is same as var.p\n",
    "print(np.var(norm_rvs,ddof=1))# this is same as var.s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=1.8389204665603247, pvalue=0.066223848288555126)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.ttest_ind(rvs1,rvs2)\n",
    "#p >alpha, alpha is significant value, 0.05, therefore we failed to reject null hypothesis , \n",
    "# null pythohtsis is rvs1 and rvs2 are equal\n",
    "# but here t-stat is negative value, shall we reject null hypothesis or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=1.8389204665603247, pvalue=0.066224350735451645)\n",
      "Ttest_indResult(statistic=-1.8389204665603247, pvalue=0.066224350735451645)\n"
     ]
    }
   ],
   "source": [
    "print(stats.ttest_ind(rvs1,rvs2,equal_var = False))\n",
    "print(stats.ttest_ind(rvs2,rvs1,equal_var = False))\n",
    "\n",
    "#for same mean/std and same size distribution, ttest result is same for assuming same variance o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=0.1341191082078752, pvalue=0.89333543091973067)\n",
      "Ttest_indResult(statistic=0.1341191082078752, pvalue=0.89334617903384461)\n"
     ]
    }
   ],
   "source": [
    "rvs3=stats.norm.rvs(loc=5, scale=20, size=500)\n",
    "\n",
    "print(stats.ttest_ind(rvs1,rvs3))\n",
    "print(stats.ttest_ind(rvs1,rvs3,equal_var = False))\n",
    "#for same mean and size distribution, scale is different. t value is same, but p value for not same variance is slightly higher, which is under-estimated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=-0.039172219990907101, pvalue=0.9687628794700589)\n",
      "Ttest_indResult(statistic=-0.03873543324538628, pvalue=0.96911404905374721)\n"
     ]
    }
   ],
   "source": [
    "rvs4=stats.norm.rvs(loc=5, scale=10, size=300)\n",
    "print(stats.ttest_ind(rvs1,rvs4))\n",
    "print(stats.ttest_ind(rvs1,rvs4,equal_var = False))\n",
    "#for same mean same scale, different size distribution, t value and p value are both different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=-0.42210654140239207, pvalue=0.68406235191764142)\n",
      "Ttest_indResult(statistic=-0.41888302290753471, pvalue=0.68898023644823225)\n"
     ]
    }
   ],
   "source": [
    "A = [0.19826790, 1.36836629, 1.37950911, 1.46951540, 1.48197798, 0.07532846]\n",
    "B = [0.6383447, 0.5271385, 1.7721380, 1.7817880]\n",
    "C = [0.6383447, 0.5271385, 1.7721380, 1.7817880,1.9,0.6]\n",
    "print(stats.ttest_ind(A,B))\n",
    "print(stats.ttest_ind(A,B,equal_var = False))\n",
    "\n",
    "#in ttest_ind, it is giving out P value for 2 tail test \n",
    "\n",
    "#when A and B get repeated for multiple times, the T value of this t test will be bigger. (absoluate value)\n",
    "#The larger the t score, the more difference there is between groups. T value 不受所作的假设检验影响\n",
    "#When you run a t test, the bigger the t-value, the more likely it is that the results are repeatable.\n",
    "#A large t-score tells you that the groups are different.\n",
    "#A small t-score tells you that the groups are similar.\n",
    "\n",
    "#A p-value is the probability that the results from your sample data occurred by chance. P-values are from 0% to 100%.\n",
    "#p value need always be less than 5%, if we want to claim this result is more sound, we need p value smaller \n",
    "\n",
    "#It goes on to say that scipy always gives the test statistic as signed. \n",
    "#This means that given p and t values from a two-tailed test, you would reject the null hypothesis of a greater-than test when p/2 < alpha and t > 0, and of a less-than test when p/2 < alpha and t < 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=-0.53554610928528656, pvalue=0.6039795132851995)\n",
      "Ttest_relResult(statistic=-0.98144998037322329, pvalue=0.37144334119334105)\n"
     ]
    }
   ],
   "source": [
    "print(stats.ttest_ind(A,C,equal_var = False))\n",
    "print(stats.ttest_rel(A,C))\n",
    "\n",
    "/*\n",
    "#Examples for the use are scores of the same set of student in different exams, \n",
    "#or repeated sampling from the same units. \n",
    "#The test measures whether the average score differs significantly across samples (e.g. exams). \n",
    "#If we observe a large p-value, for example greater than 0.05 or 0.1 then we cannot reject the null hypothesis of identical average scores. \n",
    "#If the p-value is smaller than the threshold, e.g. 1%, 5% or 10%, then we reject the null hypothesis of equal averages. \n",
    "#Small p-values are associated with large t-statistics.\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'statsmodels.stats' from 'C:\\\\Users\\\\shanchen\\\\AppData\\\\Local\\\\Enthought\\\\Canopy\\\\edm\\\\envs\\\\User\\\\lib\\\\site-packages\\\\statsmodels\\\\stats\\\\__init__.py'>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#power analysis for T-test\n",
    "\n",
    "from statsmodels import stats  as st\n",
    "# perform power analysis\n",
    "power=0.8\n",
    "alpha=0.05\n",
    "st\n",
    "#analysis = st.power.TTestIndPower()\n",
    "#result = analysis.solve_power(effect, power=power, nobs1=None, ratio=1.0, alpha=alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
